<html>
  <head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.9.0/p5.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.9.0/addons/p5.dom.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tone/13.0.1/Tone.min.js"></script>
    <link rel="stylesheet" type="text/css" href="style.css">

  </head>
  <body>
    <a class="site-title" href="https://squidgetx.github.io">
      <img src="https://squidgetx.github.io/images/Logo.svg" alt="Sylvan Zheng" height="80px" class="logo">
    </a>
    <div id='wrapper'>
    <h1>
      Inherent Tradeoffs in Fair Classification
    </h1>
    <i>
      An interactive basic introduction to statistical fairness created by <a href='https://squidgetx.github.io'>Sylvan Zheng</a>. 2020-01-22
    </i>
    <h2>
      1. Classification
    </h2>
    <p>
      When a recruiter considers a stack of resumes, s/he sorts each application into two piles - perhaps labeled "interview" and "reject".
      When a bank manager considers a series of loan applications, s/he labels each application "approve" or "reject".
      Classification, the general process of sorting cases into piles, forms the backbone of social, financial, and medical systems that
      have massive impact on people's lives.
    </p>

    <div id='sketch0' class='int'></div>
    <p class='subtitle'>
      Note: All interactive examples in this essay are randomly generated models meant to illustrate inherent relationships and concepts in
      classification systems generally. None are meant to reflect on any exact real world population or dataset.
    </p>
    <p>
      As we enter an age where data is frequently compared to powerhouse commodities like <a href = 'https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data'>oil</a>,
      many common classification tasks are being performed by complex computer systems instead of individuals.
      Some examples include:
      <ul>
        <li>
        <a href='https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing'>
          Criminal sentencing systems are being used to label defendants as likely or unlikely to commit future crimes
        </a>
        <li>
          <a href='https://news.cornell.edu/stories/2019/11/are-hiring-algorithms-fair-theyre-too-opaque-tell-study-finds'>
            Hiring systems are screening job applications
          </a>
        </li>
        <li>
          <a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4348437/'>
            Medical systems give cancer prognoses
          </a>
        </li>
      </ul>
    </p>
    <p>
      As these systems grow larger and more influential,
      it has become a special point of interest to ensure not only a high level of overall performance but also fairness along gender, race, and other protected class lines.
    </p>
    <h2>
      2. What is Fair Classification?
    </h2>

    <p>
      How can we tell if a classification is unfair?
    </p>
    <h3>
      2.1. Balanced Accuracy (Positive Prediction Rate)
    </h3>
    <p>
      Perhaps even a highly accurate cancer-prognosis system mistakenly gives more aggressive prognoses to elderly patients than to young ones,
      or a hiring system offers more interviews to unqualified men than to women.
      In these cases the system's accuracy is significantly different for one group than another, and a hiring manager or doctor using one of these systems
      might interpret recommendations differently depending on the group membership of the person under consideration.
    </p>
    <div id='sketch1' class='int'></div>
    <p class='subtitle'>
      Note: One trivial way of balancing accuracy is to exactly interview all the qualified candidates and none of the unqualified ones (or vice versa).
      Since these algorithms will never be so perfect, most discussions of fairness ignore these "degenerate" cases.
    </p>
    <h3>
      2.2. Balanced Negative And Positive Prediction Rates
    </h3>

    <p>
      You may have noticed that our measure of accuracy captures how often the algorithm decides to interview a qualified candidate,
      but ignores the times when someone who deserves an interview is rejected.

      If qualified men and women are interviewed at the same rate but many qualified women are rejected (more than the number of rejected, qualified men)
      then that system would still be unfair. After all, if you were a qualified woman it means you would then have a lower chance than an equally qualified man
      of getting an interview!
    </p>
    <p>
      In order for an algorithm to be fair it must balance both the mistakes it makes in its positive labels (interviews) as well as its negative labels (rejections).
      You may find it difficult to perfectly balance both in these small-scale examples, but see how close you can get.
    </p>
    <div id='sketch2' class='int'></div>
    <p class='subtitle'>
        Note how difficult it can be to construct a good test condition for these labels. What does it mean for a candidate to be qualified, and how do you measure it?
        If you reject someone's loan application, how do you evaluate whether or not they would have been able to pay it back?
    </p>
    <h3>
      2.3. Balanced Error Rates
    </h3>
    <p>
      So, as long as classification systems make labeling mistakes for the different groups at roughly the same rate,
      there is nothing to worry about, right?

      But another notion of fairness starts to emerge if we consider labeling mistakes within classes instead of class mistakes within labels.
    <p>
    </p>
      The terminology quickly becomes confusing, so let us return to a concrete example.
      Suppose a criminal recidivism system mistakenly labels blacks as high-risk more often than whites;
      say 10% of blacks who never go on to commit another crime are labeled as high risk whereas only 5% of whites who never commit another crime receive that same high risk label.
      This system is unfair in a distinctly different way; it punishes more blacks with an undeserved high-risk rating which could lead to harsher sentencing among other potential consequences.
    </p>
    <p class='subtitle'>
      Our earlier definition of fairness was concerned with ensuring the predictive power of the system was balanced in the two groups.
      In the criminal recidivism example, this means making sure that the "high risk" label is as accurate for whites as it is for blacks (and correspondingly, that the "low risk" label is accurate for both groups as well).
    </p>
    <div id='sketch3' class='int'></div>
    <h2>3. Inherent Tradeoffs</h2>
      <p>
        It is actually impossible to satisfy both of the main definitions of fairness we have considered so far without a perfect algorithm (or identical groups).
        It is possible to balance the label mistakes within classes, the class mistakes within labels, but not both at once.
      </p>
      <p>
        In order to lower the false positive rate
        (the probability that an unqualified candidate is given an interview, or an non-recidivist is labeled "high risk"),
        fewer unqualified candidates could be given interviews.
        This increases the positive accuracy (the probability that an interviewed candidate is qualified) as well as the
        negative accuracy (the probability that an unqualified candidate is rejected).
        The amount that all these values change depend on the the overall base rate in the population (the ratio of X's to O's, or the ratio of qualified to unqualified candidates).
        The same is true of the false negative rate.
      </p>
      <p>
        In other words, the error rates for each group depend on both the accuracy of the classification and the group's base rate.
        This implies then that if the error rates for both groups are the same, and the base rates are different, the accuracy must be imbalanced.
      </p>
      <p>
        See for yourself below:
      </p>
      <div id='sketch4' class='int'></div>
      <p class='subtitle'>If you are interested in a more precise proof of this concept, I encourage you to check out some of the
        <a href='#5'>original papers</a> on which this essay is based!</p>

    <p>
      No matter how much juggling happens, all the fairness definitions cannot be satisfied without perfect prediction or identical groups.
    </p>
    <h2>4. What Next?</h4>
    <p>
      Are all algorithmic classification systems, or even non-algorithmic classification systems for that matter,
      doomed to unfairness when the groups aren't identical?
    </p>
    <p>
      The mathematics say that the answer is unequivocally yes, something that was not so obvious during the
      <a href='https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing'>Propublica-Northpointe controversy</a> of 2016.
      But what steps are there in 2020? There is nuance yet that algorithm builders, researchers, journalists, and policymakers
      can draw on to reduce the real world impacts of the unfairness present within these systems.
      Not all unfairnesses are equal after all; aside from imbalances in error rates or accuracies there are myriad other important criteria to consider
      when evaluating these systems. How does the system evaluate success? Is it designed to correct for errors? Is the impact of error rate imbalance
      greater or less than that of accuracy imbalance?
    </p>
    <p>
      These questions aren't always easy to answer. But as we turn over the responsibilities of classification to ever more complex automated systems they
      become questions that deserve our hard consideration.
    </p>

    <img src='phx.png' />
    <p class='subtitle'>
      Osamu Tezuka's 1967 manga "Phoenix: A Tale Of The Future" depicts a human civilization whose every decision is subject to the will of the supercomputer Hallelujah.
    </p>
    <h2 id='5'>5. Appendix / Bibliography</h2>
    This essay is based on several excellent papers and articles published on the subject.
    <ul>
      <li>
        <a href='https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb'>A Tutorial on Fairness in Machine Learning</a>
      </li>
      <li>
        Chouldechova, Alexandra. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data, vol. 5, no. 2, 2017, pp. 153–163., doi:10.1089/big.2016.0047.
        (<a href='https://arxiv.org/pdf/1610.07524.pdf'>link</a>)
      </li>
      <li>
        Kleinberg, Jon. “Inherent Trade-Offs in the Fair Determination of Risk Scores", ACM SIGMETRICS Performance Evaluation Review, vol. 46, no. 1, 2019, pp. 40-40., doi:10.1145/3308809.3308832.
        (<a href='https://arxiv.org/pdf/1609.05807.pdf'>link</a>)
      </li>
      <li>
        Julia Angwin, Jeff Larson, Surya Mattu,
and Lauren Kirchner. Machine bias:
There’s software used across the country to predict future criminals. and it’s
biased against blacks. 2016.
(<a href='https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing'>link</a>)
      </li>
      <li>
        Anthony W Flores, Kristin Bechtel, and
        Christopher T Lowenkamp. False positives,
        false negatives, and false analyses: A rejoinder to “machine bias: There’s software used
        across the country to predict future criminals.
        and it’s biased against blacks.”. Unpublished
        manuscript, 2016.
      </li>
      <li>
        Julia Angwin and Jeff Larson. Bias in Criminal Risk Scores Is Mathematically Inevitable, Researchers Say 2016.
        (<a href='https://www.propublica.org/article/bias-in-criminal-risk-scores-is-mathematically-inevitable-researchers-say'>link</a>)
      </li>
    </ul>

  </div>
    <script src="script.js"></script>
  </body>
</html>
